4th 
  
import torch, torchvision
import torch.nn as nn
from torch.utils.data import DataLoader

# Dataset
t = torchvision.transforms.ToTensor()
train = DataLoader(torchvision.datasets.MNIST('./data', train=True, download=True, transform=t), batch_size=64, shuffle=True)
test = DataLoader(torchvision.datasets.MNIST('./data', train=False, download=True, transform=t), batch_size=64)

# Model
class Net(nn.Module):
    def __init__(self, bn=False, do=False):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.bn = nn.BatchNorm1d(256) if bn else nn.Identity()
        self.do = nn.Dropout(0.5) if do else nn.Identity()
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = torch.relu(self.bn(self.fc1(x.view(-1, 784))))
        return self.fc2(self.do(x))

# Train & Evaluate
def run(model):
    opt = torch.optim.Adam(model.parameters(), lr=1e-3)
    loss_fn = nn.CrossEntropyLoss()
    for _ in range(5):
        for x, y in train:
            opt.zero_grad()
            loss_fn(model(x), y).backward()
            opt.step()
    correct = sum((model(x).argmax(1) == y).sum().item() for x, y in test)
    return 100 * correct / len(test.dataset)

# Results
print(f"No BN/Dropout: {run(Net()):.2f}%")
print(f"With BatchNorm: {run(Net(bn=True)):.2f}%")
print(f"With Dropout: {run(Net(do=True)):.2f}%")






5th
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# Device support
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 1. Data Loading
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# 2. CNN Definition
class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.relu = nn.ReLU()
        self._to_linear = None  # Will be set in forward

        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # 32x32 → 16x16 → 8x8
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        if self._to_linear is None:
            self._to_linear = x.view(x.size(0), -1).shape[1]
        x = x.view(-1, self._to_linear)
        x = self.relu(self.fc1(x))
        return self.fc2(x)

# 3. Train & Evaluate Function
def train_and_evaluate(model, optimizer, criterion, epochs=5):
    model.to(device)
    train_losses, test_losses, test_accuracies = [], [], []

    for epoch in range(epochs):
        model.train()
        running_train_loss = 0.0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_train_loss += loss.item()
        train_losses.append(running_train_loss / len(train_loader))

        model.eval()
        running_test_loss, correct_predictions, total_samples = 0.0, 0, 0
        with torch.no_grad():
            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                running_test_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                correct_predictions += (predicted == labels).sum().item()
                total_samples += labels.size(0)

        test_losses.append(running_test_loss / len(test_loader))
        accuracy = 100 * correct_predictions / total_samples
        test_accuracies.append(accuracy)

        print(f"Epoch [{epoch+1}/{epochs}], "
              f"Train Loss: {train_losses[-1]:.4f}, "
              f"Test Loss: {test_losses[-1]:.4f}, "
              f"Accuracy: {accuracy:.2f}%")

    return train_losses, test_losses, test_accuracies

# 4. Setup Optimizers & Experiments
criterion = nn.CrossEntropyLoss()
experiments = {
    "SGD": {
        "model": SimpleCNN(),
        "optimizer": None,
        "train_losses": [],
        "test_losses": [],
        "accuracies": []
    },
    "Adam": {
        "model": SimpleCNN(),
        "optimizer": None,
        "train_losses": [],
        "test_losses": [],
        "accuracies": []
    }
}

experiments["SGD"]["optimizer"] = optim.SGD(experiments["SGD"]["model"].parameters(), lr=0.01, momentum=0.9)
experiments["Adam"]["optimizer"] = optim.Adam(experiments["Adam"]["model"].parameters(), lr=0.001)

# Run training
for name, exp_data in experiments.items():
    print(f"\nTraining with {name} Optimizer:")
    exp_data["train_losses"], exp_data["test_losses"], exp_data["accuracies"] = \
        train_and_evaluate(exp_data["model"], exp_data["optimizer"], criterion)

# 5. Plotting
epochs_range = range(1, 6)
plt.figure(figsize=(12, 5))

# Loss Plot
plt.subplot(1, 2, 1)
plt.plot(epochs_range, experiments["SGD"]["train_losses"], label='SGD - Train Loss')
plt.plot(epochs_range, experiments["SGD"]["test_losses"], label='SGD - Test Loss')
plt.plot(epochs_range, experiments["Adam"]["train_losses"], label='Adam - Train Loss')
plt.plot(epochs_range, experiments["Adam"]["test_losses"], label='Adam - Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Comparison')
plt.legend()

# Accuracy Plot
plt.subplot(1, 2, 2)
plt.plot(epochs_range, experiments["SGD"]["accuracies"], label='SGD - Accuracy')
plt.plot(epochs_range, experiments["Adam"]["accuracies"], label='Adam - Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy Comparison')
plt.legend()

plt.tight_layout()
plt.show()







6th
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt
import numpy as np

# Load and preprocess MNIST
(x_train, _), (x_test, _) = mnist.load_data()
x_train, x_test = x_train[..., None] / 255.0, x_test[..., None] / 255.0

# Simulate blurry inputs
def blur_images(images):
    return tf.image.resize(tf.image.resize(images, (14, 14)), (28, 28))

x_train_blur = blur_images(x_train)
x_test_blur = blur_images(x_test)

# Simple U-Net model
def simple_unet(input_shape):
    inputs = layers.Input(shape=input_shape)
    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)
    x = layers.MaxPooling2D()(x)
    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)
    x = layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu')(x)
    outputs = layers.Conv2D(1, 1, activation='sigmoid')(x)
    return models.Model(inputs, outputs)

# Compile and train
model = simple_unet((28, 28, 1))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(x_train_blur, x_train, epochs=5, batch_size=64, validation_split=0.1)

# Predict on test samples
predicted = model.predict(x_test_blur[:5])

# Visualization
for i in range(5):
    plt.figure(figsize=(8, 2))
    for j, (title, img) in enumerate(zip(
        ["Blurry Input", "Ground Truth", "Predicted Mask"],
        [x_test_blur[i], x_test[i], predicted[i]]
    )):
        plt.subplot(1, 3, j + 1)
        plt.title(title)
        plt.imshow(np.squeeze(img), cmap='gray')
        plt.axis('off')
    plt.show()




9th
import torch, torch.nn as nn, torch.optim as optim

# Simple RNN model for binary classification
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.rnn(x)       # (batch, seq_len, hidden_size)
        return self.fc(out[:, -1]) # Use last time step's output

# Hyperparameters
input_size, hidden_size, output_size = 8, 32, 1
seq_len, batch_size = 10, 16

# Random input and target
X = torch.randn(batch_size, seq_len, input_size)
y = torch.randint(0, 2, (batch_size, 1)).float()

# Model, loss, optimizer
model = SimpleRNN(input_size, hidden_size, output_size)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(10):
    outputs = model(X)
    loss = criterion(outputs, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(f"Epoch [{epoch+1}/10] Loss: {loss.item():.4f}")





10th
import torch
import torch.nn as nn
import torch.optim as optim

# Prepare character-level data
text = "hello world, this is a simple text generation using LSTMs."
chars = sorted(set(text))
char2idx = {c: i for i, c in enumerate(chars)}
idx2char = {i: c for c, i in char2idx.items()}
seq_len = 10

# Create input-output sequences
X = [[char2idx[c] for c in text[i:i+seq_len]] for i in range(len(text) - seq_len)]
Y = [char2idx[text[i+seq_len]] for i in range(len(text) - seq_len)]
X, Y = torch.tensor(X), torch.tensor(Y)

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
X, Y = X.to(device), Y.to(device)

# Define LSTM model
class TextLSTM(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x):
        x = self.embed(x)
        x, _ = self.lstm(x)
        return self.fc(x[:, -1])  # Only last output

# Model, loss, optimizer
vocab_size, embed_size, hidden_size = len(chars), 16, 128
model = TextLSTM(vocab_size, embed_size, hidden_size).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training loop
for epoch in range(50):
    model.train()
    optimizer.zero_grad()
    output = model(X)
    loss = criterion(output, Y)
    loss.backward()
    optimizer.step()
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

# Text generation function (argmax sampling)
def generate(start, length=50):
    model.eval()
    seq = [char2idx[c] for c in start]
    result = start
    for _ in range(length):
        inp = torch.tensor([seq], dtype=torch.long).to(device)
        with torch.no_grad():
            logits = model(inp)
            pred = torch.argmax(logits, dim=1).item()
        result += idx2char[pred]
        seq = seq[1:] + [pred]
    return result

# Output
print("\nGenerated Text:\n", generate("hello wor", 50))







11th
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super().__init__()
        assert embed_size % heads == 0
        self.head_dim = embed_size // heads
        self.heads = heads
        self.embed_size = embed_size

        self.values = nn.Linear(embed_size, embed_size)
        self.keys = nn.Linear(embed_size, embed_size)
        self.queries = nn.Linear(embed_size, embed_size)
        self.fc_out = nn.Linear(embed_size, embed_size)

    def forward(self, value, key, query, mask=None):
        N, seq_len = query.shape[0], query.shape[1]

        def transform(x, proj):
            x = proj(x)
            return x.view(N, -1, self.heads, self.head_dim).transpose(1, 2)

        V, K, Q = map(lambda x, p: transform(x, p), [value, key, query], [self.values, self.keys, self.queries])

        energy = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))

        attention = torch.softmax(energy, dim=-1)
        out = (attention @ V).transpose(1, 2).reshape(N, seq_len, self.embed_size)
        return self.fc_out(out)

# Example usage
attention = MultiHeadAttention(embed_size=128, heads=8)
x = torch.rand(2, 10, 128)  # (batch, seq_len, embed_dim)
output = attention(x, x, x)
print(output.shape)  # → torch.Size([2, 10, 128])





12
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load and transform MNIST data
transform = transforms.ToTensor()
train_data = Subset(datasets.MNIST('./data', train=True, download=True, transform=transform), range(200))
test_data = Subset(datasets.MNIST('./data', train=False, download=True, transform=transform), range(50))
train_loader = DataLoader(train_data, batch_size=10, shuffle=True)
test_loader = DataLoader(test_data, batch_size=10)

# Define Autoencoder model
class AutoEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28*28, 256), nn.ReLU(),
            nn.Linear(256, 64), nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(64, 256), nn.ReLU(),
            nn.Linear(256, 28*28), nn.Sigmoid()
        )

    def forward(self, x):
        return self.decoder(self.encoder(x))

model = AutoEncoder().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# Train the model
for epoch in range(10):
    model.train()
    train_loss = 0.0
    for img, _ in train_loader:
        img = img.view(img.size(0), -1).to(device)
        loss = criterion(model(img), img)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    
    # Evaluate
    model.eval()
    test_loss = sum(criterion(model(img.view(img.size(0), -1).to(device)), img.view(img.size(0), -1).to(device)).item()
                    for img, _ in test_loader)

    print(f"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.4f}, Test Loss: {test_loss/len(test_loader):.4f}")
